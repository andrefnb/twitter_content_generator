{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_MODEL_NAME = \"bible_model.sav\"\n",
    "DATA_FILE_NAME = \"bible.txt\"\n",
    "TWEETS_FILE_NAME = \"tweets_storage.txt\"\n",
    "NR_OF_TRAINING_CHARACTERS = 1000000\n",
    "NR_UNITS = 300\n",
    "def load_data():\n",
    "\n",
    "    # load text\n",
    "    filename = DATA_FILE_NAME\n",
    "    text = (open(filename).read())\n",
    "\n",
    "    tweets_file_name = TWEETS_FILE_NAME\n",
    "    text = (open(filename).read()) + \"\\n\" + text\n",
    "\n",
    "    text = text[0:NR_OF_TRAINING_CHARACTERS]\n",
    "    text = text.lower()\n",
    "    # print(text)\n",
    "\n",
    "    # mapping characters with integers\n",
    "    unique_chars = sorted(list(set(text)))\n",
    "\n",
    "    char_to_int = {}\n",
    "    int_to_char = {}\n",
    "\n",
    "    for i, c in enumerate(unique_chars):\n",
    "        char_to_int.update({c: i})\n",
    "        int_to_char.update({i: c})\n",
    "\n",
    "    # print(char_to_int)\n",
    "    # print(int_to_char)\n",
    "\n",
    "    # preparing input and output dataset\n",
    "    X = []\n",
    "    Y = []\n",
    "    number_period = 50\n",
    "\n",
    "    for i in range(0, len(text) - number_period, 1):\n",
    "        sequence = text[i:i + number_period]\n",
    "        label = text[i + number_period]\n",
    "        X.append([char_to_int[char] for char in sequence])\n",
    "        Y.append(char_to_int[label])\n",
    "\n",
    "    # print(\"X\", X)\n",
    "    # print(\"Y\", Y)\n",
    "\n",
    "    # reshaping, normalizing and one hot encoding\n",
    "    X_modified = np.reshape(X, (len(X), number_period, 1))\n",
    "    # print(\"X_modified\", X_modified)\n",
    "    X_modified = X_modified / float(len(unique_chars))\n",
    "    # print(\"X_modified\", X_modified)\n",
    "    Y_modified = np_utils.to_categorical(Y)\n",
    "    # print(\"Y_modified\", Y_modified)\n",
    "\n",
    "    return unique_chars, int_to_char, char_to_int, X, Y, X_modified, Y_modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_modified, Y_modified, load_checkpoint=False):\n",
    "\n",
    "    # defining the LSTM model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(NR_UNITS, input_shape=(X_modified.shape[1], X_modified.shape[2]), return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(NR_UNITS))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(Y_modified.shape[1], activation='softmax'))\n",
    "\n",
    "    # Checkpoint loading\n",
    "    if load_checkpoint:\n",
    "        model.load_weights(\"bible_weights_improvement.hdf5\")\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "    epoch_number = 8\n",
    "\n",
    "    # Checkpointing\n",
    "    filepath = \"bible_weights_improvement.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "    callbacks_list = [checkpoint]\n",
    "\n",
    "    # fitting the model\n",
    "    model.fit(X_modified, Y_modified, epochs=epoch_number, batch_size=16, validation_split=0.33, callbacks=callbacks_list)\n",
    "\n",
    "    filename = FILE_MODEL_NAME\n",
    "    save_model(filename, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "\n",
    "print(\"Loading data\")\n",
    "unique_chars, int_to_char, char_to_int, X, Y, X_modified, Y_modified = load_data()\n",
    "print(\"Training began\")\n",
    "train_model(X_modified, Y_modified, load_checkpoint = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
